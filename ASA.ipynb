{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0663fa0c",
   "metadata": {},
   "source": [
    "# Arabic Tweets Sentiment Anaylsis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d263096",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Dataset\n",
    "\n",
    "* This dataset was collected to provide Arabic sentiment corpus for the research community to investigate deep learning approaches for Arabic sentiment analysis.\n",
    "\n",
    "* This dataset we collected in April 2019. It contains 58K Arabic tweets (47K training, 11K test) tweets annotated in positive and negative labels. The dataset is balanced and collected using positive and negative emojis lexicon.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de46e3",
   "metadata": {},
   "source": [
    "## Exploring and downloading the Text data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0e5178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f4fc38",
   "metadata": {},
   "source": [
    "### Train Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data_negative = pd.read_csv('data/train_Arabic_tweets_negative_20190413.tsv',sep='\\t',header=None, names=['sentiment','tweets'], encoding='utf-8')\n",
    "tweets_data_positive = pd.read_csv('data/train_Arabic_tweets_positive_20190413.tsv',sep='\\t',header=None, names=['sentiment','tweets'], encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024846a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data_negative.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98534d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data_positive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcc12fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data_negative.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93874708",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data_positive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bfaf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([tweets_data_negative,tweets_data_positive], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9459ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c1e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc856575",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c199f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('fulldf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c26011",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d9975",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed5733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf46b812",
   "metadata": {},
   "source": [
    "### Test Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62810cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets_negative = pd.read_csv('data/test_Arabic_tweets_negative_20190413.tsv',sep='\\t',header=None,  names=['sentiment','tweets'], encoding='utf-8')\n",
    "test_tweets_positive = pd.read_csv('data/test_Arabic_tweets_positive_20190413.tsv',sep='\\t',header=None,  names=['sentiment','tweets'], encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d166fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([test_tweets_negative,test_tweets_positive], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1777192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae5c3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef004a1",
   "metadata": {},
   "source": [
    "## Data Exploration and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df917c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import re as regex\n",
    "import plotly\n",
    "from plotly import graph_objs\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "#from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import cufflinks as cf\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b371aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='sentiment',data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bfe1aa",
   "metadata": {},
   "source": [
    "### cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42911eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "punctuatuions = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation\n",
    "#nltk.download('stopwords')\n",
    "stopwords=nltk.corpus.stopwords.words(\"arabic\")\n",
    "\n",
    "arabic_diacritics = regex.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", regex.VERBOSE)\n",
    "\n",
    "def clean_tweets(tweet):\n",
    "    #remove punctuations\n",
    "    translator = str.maketrans('','',punctuatuions)\n",
    "    tweet = tweet.translate(translator)\n",
    "    \n",
    "    #stopwords\n",
    "    tweet = ' '.join(word for word in tweet.split() if word not in stopwords)\n",
    "    \n",
    "    #remove longation\n",
    "    tweet = regex.sub(\"[إأآا]\", \"ا\", tweet)\n",
    "    tweet = regex.sub(\"ى\", \"ي\", tweet)\n",
    "    tweet = regex.sub(\"ؤ\", \"ء\", tweet)\n",
    "    tweet = regex.sub(\"ئ\", \"ء\", tweet)\n",
    "    tweet = regex.sub(\"ة\", \"ه\", tweet)\n",
    "    tweet = regex.sub(\"گ\", \"ك\", tweet)\n",
    "    \n",
    "    # remove URL\n",
    "    tweet = regex.sub('http\\S+\\s*', ' ', tweet)\n",
    "    \n",
    "    # Remove usernames\n",
    "    tweet = regex.sub(r\"@[^\\s]+[\\s]?\",'',tweet)\n",
    "    \n",
    "    # remove special characters \n",
    "    tweet = regex.sub(\"@[ا-ي0-9]+\", \" \", tweet)\n",
    "    tweet = regex.sub(\"[أ-ي]#+\", \" \", tweet)\n",
    "    tweet = regex.sub(\"#[أ-ي]+\", \" \", tweet)\n",
    "    \n",
    "    # remove Numbers\n",
    "    tweet = regex.sub('^[\\u0621-\\u064A\\u0660-\\u0669 ]+$', '', tweet)\n",
    "    tweet = regex.sub('\\.+', '', tweet)\n",
    "    tweet = regex.sub(':', '', tweet)\n",
    "    tweet = regex.sub('!', '', tweet)\n",
    "    tweet = regex.sub('،','',tweet)\n",
    "    tweet = regex.sub('-','',tweet)\n",
    "    tweet = regex.sub('_','',tweet)\n",
    "    \n",
    "    # remove Tashkeel\n",
    "    tweet = regex.sub(arabic_diacritics, '', tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8b91a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweets'] = df['tweets'].apply(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df8ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f570d5",
   "metadata": {},
   "source": [
    "## Tokenization And Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ccf5b",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b08483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import  word_tokenize\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "df['tweets'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143ce79",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81981e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "def stemming(words):\n",
    "    stem_words = []\n",
    "    for word in words:\n",
    "        word = ps.stem(word)\n",
    "        stem_words.append(word)\n",
    "    return stem_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc62a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tokenized'] =df['tweets'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0df2b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stemmed'] = df['tweets'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21fc66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02323b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e77798",
   "metadata": {},
   "source": [
    "##  bag-of-words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq = Counter()\n",
    "for idx in df.index:\n",
    "    words_freq.update(df.loc[idx,\"Tokenized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0784a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c254347",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c63af",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be67ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_list(processed_data):\n",
    "    #print(processed_data)\n",
    "    min_occurrences=3 \n",
    "    max_occurences=500 \n",
    "    stopwords=nltk.corpus.stopwords.words(\"arabic\")\n",
    "    wordlist = []\n",
    "    \n",
    "    words_freq = Counter()\n",
    "    for idx in processed_data.index:\n",
    "        words_freq.update(processed_data.loc[idx, \"Tokenized\"])\n",
    "\n",
    "    word_df = pd.DataFrame(data={\"word\": [k for k, w in words_freq.most_common() if min_occurrences < w < max_occurences],\n",
    "                                 \"occurrences\": [w for k, w in words_freq.most_common() if min_occurrences < w < max_occurences]\n",
    "                                },\n",
    "                           columns=[\"word\", \"occurrences\"])\n",
    "    #print(word_df)\n",
    "    word_df.to_csv(\"wordlist.csv\", index_label=\"idx\")\n",
    "    wordlist = [k for k, w in words_freq.most_common() if min_occurrences < w < max_occurences]\n",
    "    #print(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e21767",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1bf697",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.read_csv(\"wordlist.csv\", encoding=\"utf8\")\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76cef99",
   "metadata": {},
   "source": [
    "## Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc6b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Length of tweets\n",
    "df['Tweets_len'] = df['Tokenized'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed951b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b24ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for emotion in df['sentiment'].unique():\n",
    "    df[df['sentiment']==emotion]['Tweets_len'].plot(\n",
    "      kind='hist',\n",
    "      bins=20,\n",
    "      title='Length')\n",
    "    plt.xlabel(f'Text length for {emotion} emotion')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29653c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df[\"Tokenized\"].apply(lambda x: len(str(x).split(\" \")))\n",
    "df['char_count'] = df[\"Tokenized\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\n",
    "df['sentence_count'] = df[\"Tokenized\"].apply(lambda x: len(str(x).split(\".\")))\n",
    "df['avg_word_length'] = df['char_count'] / df['word_count']\n",
    "df['avg_sentence_lenght'] = df['word_count'] / df['sentence_count']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11741e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9c7178",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = df.groupby('sentiment').mean()\n",
    "sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cceb45",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentiments.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ed875",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(sentiments.corr(),cmap='coolwarm',annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d39b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('sentence_count',\n",
    "  axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07199bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = df.groupby('sentiment').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b008d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(sentiments.corr(),cmap='coolwarm',annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f26f722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = pd.read_csv(\"wordlist.csv\", encoding=\"utf8\")\n",
    "# words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0483cd27",
   "metadata": {},
   "source": [
    "## After cleaning :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f751482",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleaned_df = df[['sentiment','tweets']]\n",
    "df.to_csv('cleaned_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c00ae6",
   "metadata": {},
   "source": [
    "## ML Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da8f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import log_loss\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5134bf3c",
   "metadata": {},
   "source": [
    "### Logistic regression using TfidfVectorizer on tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b52799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into featuers and target\n",
    "feature = df.tweets\n",
    "target = df['sentiment']\n",
    "# splitting into train and tests\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(feature, target, test_size =.2, random_state=100)\n",
    "\n",
    "# make pipeline\n",
    "pipe = make_pipeline(TfidfVectorizer(),\n",
    "                    LogisticRegression())\n",
    "# make param grid\n",
    "param_grid = {'logisticregression__C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# create and fit the model\n",
    "model = GridSearchCV(pipe, param_grid, cv=5)\n",
    "model.fit(X_train,Y_train)\n",
    "\n",
    "# make prediction and print accuracy\n",
    "prediction = model.predict(X_test)\n",
    "print(f\"Accuracy score is {accuracy_score(Y_test, prediction):.2f}\")\n",
    "print(classification_report(Y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d3e06",
   "metadata": {},
   "source": [
    "### Logistic regression using text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8c1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xt = df.loc[:,'Tweets_len':'avg_sentence_lenght']\n",
    "Xt = df.loc[:,['avg_word_length','avg_sentence_lenght']]\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42938557",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xt_train, Xt_test, y_train, y_test = train_test_split(Xt, y, test_size = .2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd47dcb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(Xt_train,y_train)\n",
    "prediction = lr_model.predict(Xt_test)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e48c706",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "sentiment_model = MultinomialNB().fit(Xt_train, y_train)\n",
    "prediction = sentiment_model.predict(Xt_test)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f241d2a",
   "metadata": {},
   "source": [
    "### Logistic regression using TfidfVectorizer and BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69000cf6",
   "metadata": {},
   "source": [
    "#### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c51f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#get dataset bag-of-words counts as a vector\n",
    "bow_transformer = CountVectorizer(analyzer=tokenize).fit(df['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f0a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW vector representation\n",
    "messages_bow = bow_transformer.transform(df['tweets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b20cc5",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2e12ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the entire bag-of-words corpus into TF-IDF corpus \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63ebe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "print(messages_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b032b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.loc[:,[\"tweets\",'avg_word_length','avg_sentence_lenght']]\n",
    "X=messages_tfidf\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48af94b9",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4176f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_ = LogisticRegression()\n",
    "lr_.fit(X_train,y_train)\n",
    "prediction = lr_.predict(X_test)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79db8e6a",
   "metadata": {},
   "source": [
    "#### MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f57eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model_ = MultinomialNB().fit(X_train, y_train)\n",
    "prediction = sentiment_model_.predict(X_test)\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd29f63",
   "metadata": {},
   "source": [
    "## DL Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ea25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Basics\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# gensim\n",
    "import gensim\n",
    "\n",
    "# keras\n",
    "np.random.seed(13)\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Dense, Embedding, Reshape, Activation, \n",
    "                          SimpleRNN, LSTM, Convolution1D, \n",
    "                          MaxPooling1D, Dropout, Bidirectional, SpatialDropout1D)\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.datasets import imdb, reuters\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop\n",
    "\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0997dcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original data\n",
    "data = pd.read_csv('fulldf.csv', encoding='utf-8')\n",
    "df = pd.read_csv('cleaned_df.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0eba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encoded the target column\n",
    "lb=LabelEncoder()\n",
    "data_v1 = df[['tweets','sentiment']]\n",
    "\n",
    "data_v1['sentiment'] = lb.fit_transform(df['sentiment'])\n",
    "#data_v1.sentiment=data_v1.sentiment.astype(str)\n",
    "\n",
    "data_v1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6647a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing and converting the tweets into numerical vectors.\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "data_v1.tweets=data_v1.tweets.astype(str)\n",
    "tokenizer = Tokenizer(num_words=500, split=' ') \n",
    "tokenizer.fit_on_texts(data_v1['tweets'].values)\n",
    "X = tokenizer.texts_to_sequences(data_v1['tweets'].values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6115218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_v1['sentiment']\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a744c664",
   "metadata": {},
   "source": [
    "\n",
    "### Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd388009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create LSTM model with keras\n",
    "embedding_dim = 100\n",
    "dropout = 0.5\n",
    "opt = 'adam'\n",
    "#clear_session()\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=500, \n",
    "                           output_dim=100, \n",
    "                           input_length=X.shape[1]))\n",
    "model.add(layers.Bidirectional(layers.LSTM(100, dropout=0.5, \n",
    "                                           recurrent_dropout=0.5, \n",
    "                                           return_sequences=True)))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(dropout))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dropout(dropout))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dropout(dropout))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=opt, \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8147b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y=pd.get_dummies(data_v1['sentiment'])\n",
    "X_train, X_test, label_train, label_test = train_test_split(X, data_v1['sentiment'], test_size=0.3, random_state=42)\n",
    "print(\"Training:\", len(X_train))\n",
    "print(\"Testing: \", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe7784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, label_train, epochs = 5, batch_size=32 , verbose = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e25009",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,label_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a696385",
   "metadata": {},
   "source": [
    "# Word embedding + LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b7009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Basics\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# gensim\n",
    "import gensim\n",
    "\n",
    "# keras\n",
    "np.random.seed(13)\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Dense, Embedding, Reshape, Activation, \n",
    "                          SimpleRNN, LSTM, Convolution1D, \n",
    "                          MaxPooling1D, Dropout, Bidirectional, SpatialDropout1D)\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.datasets import imdb, reuters\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop\n",
    "\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038d2582",
   "metadata": {},
   "source": [
    "### Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbcebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# The original data\n",
    "data = pd.read_csv('fulldf.csv', encoding='utf-8')\n",
    "\n",
    "# Encoded the target column\n",
    "lb=LabelEncoder()\n",
    "df = pd.read_csv('cleaned_df.csv', encoding='utf-8')\n",
    "df = df[['tweets','sentiment']]\n",
    "df['sentiment'] = lb.fit_transform(df['sentiment'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31cb524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#More cleaning\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "\n",
    "tweets_lines = list()\n",
    "\n",
    "lines = data['tweets'].values.tolist() #convert original lines to a list\n",
    "\n",
    "lines_ = df['tweets'].values.tolist() #convert cleaned lines to a list\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "for line in lines_: #this loop will tokenize, strip the punctuation, remove token that are not alphabetic, remove stop words also\n",
    "    \n",
    "    #line = remove_emoji(str(line)) # no emoji\n",
    "    \n",
    "    #word tokenize \n",
    "    #tokens = word_tokenize(line)\n",
    "    tt = TweetTokenizer()\n",
    "    tokens = tt.tokenize(str(line))\n",
    "    \n",
    "    \n",
    "    tweets_lines.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcfabe2",
   "metadata": {},
   "source": [
    "### Download Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53892f1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "embedding_dim = 100 #specify dimensions of embeddings\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences=tweets_lines,\n",
    "                               vector_size = embedding_dim, window=5,\n",
    "                               min_count=1) #list of sentances (tokens)\n",
    "words = list(model.wv.index_to_key) #vocab size\n",
    "\n",
    "print('Vocabulary size: %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9eb006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model\n",
    "# sample = w2v.wv[\"حسن\"]\n",
    "# print(sample.shape)\n",
    "# #print(sample)\n",
    "# print(w2v.wv.most_similar(\"حسن\"))\n",
    "sample = model.wv[\"حسن\"]\n",
    "print(sample.shape)\n",
    "#print(sample)\n",
    "print(model.wv.most_similar(\"حسن\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5a603e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Save model in ASCII (word2vec) format\n",
    "from gensim.models import Word2Vec\n",
    "filename = 'w2v_embedding_word2vec_result.txt'\n",
    "\n",
    "model.wv.save_word2vec_format(filename,binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbdd928",
   "metadata": {},
   "source": [
    "## Load the embeddings from the file into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c57c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "embedding_index = {} #embeddings in a dict\n",
    "\n",
    "f = open('w2v_embedding_word2vec_result.txt',encoding='utf-8')\n",
    "\n",
    "for line in f:\n",
    "  values = line.split() #split each line\n",
    "  word = values[0] #first vector is always word\n",
    "  coefs = np.asarray(values[1:])\n",
    "\n",
    "  embedding_index[word]=coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ee5423",
   "metadata": {},
   "source": [
    "## **Splitting the data** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62abc75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "validation_split = 0.2\n",
    "\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(tweets_lines)\n",
    "\n",
    "sequences = tokenizer_obj.texts_to_sequences(tweets_lines)\n",
    "\n",
    "# to keep text constant, we will pad them with extra zeros (we will keep max 100)\n",
    "\n",
    "max_length = 100\n",
    "\n",
    "word_index = tokenizer_obj.word_index\n",
    "\n",
    "tweets_pad = pad_sequences(sequences,maxlen=max_length)\n",
    "sentiment = df['sentiment'].values\n",
    "\n",
    "print(tweets_pad.shape)\n",
    "print(sentiment.shape)\n",
    "\n",
    "# indices = np.arange(tweets_pad.shape[0]) #shuffle data before spliting\n",
    "# np.random.shuffle(indices)\n",
    "\n",
    "# tweets_pad = tweets_pad[indices]\n",
    "# sentiment = sentiment[indices]\n",
    "\n",
    "# num_validation_samples = int(validation_split*tweets_pad.shape[0])\n",
    "\n",
    "# x_val_pad = tweets_pad[:num_validation_samples]\n",
    "# y_val_pad = sentiment[:num_validation_samples]\n",
    "\n",
    "# X_train_pad = tweets_pad[num_validation_samples:]\n",
    "# y_train = sentiment[num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cc5fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_split = 0.1\n",
    "\n",
    "# num_test_samples = int(test_split*X_train_pad.shape[0])\n",
    "# X_test_pad = X_train_pad[:num_test_samples]\n",
    "# y_test = y_train[:num_test_samples]\n",
    "\n",
    "# X_train_pad = X_train_pad[num_test_samples:]\n",
    "# y_train = y_train[num_test_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf270330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train_pad.shape)\n",
    "# print(y_train.shape)\n",
    "\n",
    "# print(x_val_pad.shape)\n",
    "# print(y_val_pad.shape)\n",
    "\n",
    "# print(X_test_pad.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d253df43",
   "metadata": {},
   "source": [
    "### Creating an Embedding Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c190f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming embeddings (dictioanry) into matrix as Keras accept it.\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words,embedding_dim))\n",
    "\n",
    "for word, i in word_index.items(): #for each word and its values\n",
    "\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c53745c",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "Defining the Model\n",
    "\n",
    "In Keras, A **Sequential model** is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n",
    "1. Load pre-trained word embeddings into an Embedding layer\n",
    "\n",
    "2. Adding LSTM Layer\n",
    "\n",
    "3. Adding Dense Layer\n",
    "\n",
    "**model.compile** used to Configures the model for training.\n",
    "\n",
    "\n",
    "**model.add()** function is used to add layers to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6b632",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(units=32, dropout=0.2, recurrent_dropout=0.2 )) #dropout to deactivate some of the neurons to make sure that it doesn't overfit model at every epoch \n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "print('Summary of the built model..')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8177825c",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Use the .fit method.\n",
    "\n",
    "Assign the model.fit() method to a variable, which will store the Training, Validation Loss and Accuracy for each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a9b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "labels = df['sentiment']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(tweets_pad, labels, test_size= 0.3, random_state = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb55d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_no_val = model.fit(X_train,Y_train,batch_size=32,epochs=5,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efc1c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_pad,y_train,batch_size=128,epochs=10, \\\n",
    "                    validation_data=(x_val_pad,y_val_pad),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1491ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(X_test_pad,y_test,batch_size=128)\n",
    "\n",
    "print('Test Score: ', score)\n",
    "print('Test accuracy: ',acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
